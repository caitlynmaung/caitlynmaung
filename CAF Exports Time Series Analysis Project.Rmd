---
title: "R Notebook"
output: html_notebook
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

```{r}
library(ggplot2)
library(tidyverse)
library(astsa)
library(forecast)
load("/Users/caitlynmaung/Downloads/Data_File_finalproject.Rdata")
data = finalPro_data
data
```
# Initial EDA 

```{r}
plot(data$Year, data$Exports, type="o", ylab="Exports (% change from previous year)", xlab="Year", main="Export Values, 1960-2017")
```
Fluctuations in exports were their lowest 2010-2017, and peaked around 1968.

We see that the time series is not stationary because there is mostly an upward trend.
```{r}
summary(data)
```

# ACF and PACF of raw data
```{r}
export_ts <- data.frame("Year"=data$Year, "Exports"=data$Exports)
acf2(export_ts$Exports, 25, main="Non-Transformed Export Data")['ACF']
acf2(log(export_ts$Exports), 25, main="Log Transformed Export Data")['ACF']
```
We experience a slow decay of the ACF indicating that the data may not be stationary. When we take a look at the data graphed as a time series, we can see that it shows a trend of decreasing as time progresses. Because of the reasons listed above, we may need to both transform and difference the data in order to turn it into a form that is suitable for fitting to time series models.

# QQ Plot of Export Data Residuals
```{r}
qqnorm(export_ts$Exports, main="Q-Q Plot of Std Residuals")
qqline(export_ts$Exports)
```
```{r}
hist(export_ts$Exports, probability = TRUE, main = "Histogram with Density Curve", xlab = "Values", col = "lightblue")
lines(density(export_ts$Exports), col = "red", lwd = 2)
# Add a normal distribution curve
```

#ACF and PACF of Export Residuals
```{r}
model = lm(Exports ~ Year, export_ts)
acf2(residuals(model), main = "ACF and PACF of Residuals")
```
The ACF and PACF of the residuals show significant values up to lag 3, which indicates that the series is non-stationary. 

```{r}

ts.plot(ts(export_ts$Exports, frequency=1, start = 1960), ylab="Exports")
```
The time series plot confirms that the original time series is not stationary, so we will perform a Box-Cox transformation. 

# Box-Cox Transformation
```{r}
library(MASS)
model <- lm(Exports ~ Year, export_ts)

# Apply Box-Cox transformation to the response variable
boxcox_result = boxcox(model)
```

```{r}
opt_lambda <- boxcox_result$x[which.max(boxcox_result$y)]
print(opt_lambda) 
```

The optimal lambda value is about 0.5, where the log-likelihood is maximized.

```{r}
# Apply the Box-Cox transformation to the y variable with the optimal lambda value
y = export_ts$Exports
x = export_ts$Year
if (opt_lambda == 0) {
  transformed_exports <- log(y)
} else {
  transformed_exports <- (y^opt_lambda - 1) / opt_lambda
}

# Fit a model to the transformed data
transformed_model <- lm(transformed_exports ~ x, export_ts)
transformed_model
# Print out the transformed export values
transformed_exports
```
```{r}
qqnorm(transformed_exports, main="Q-Q Plot of Std Residuals")
qqline(transformed_exports)
```

```{r}
# Plot ACF and PACF of the residuals of the transformed model
acf2(residuals(transformed_model), main="ACF and PACF of Box-Cox Transformed Export Residuals")
```

The ACF decays to zero and negative values after lag 3, which indicates that the influence of past observations fades away after 3 lags. The PACF has a significant spike at lag 1, which suggests a direct AR(1) relationship in the data and probably a weaker dependence with lags 2, 3, etc. However, we will add more AR terms and model the series with an AR(3) process. This also indicates that the series is still not stationary.

```{r}
ts.plot(transformed_exports)
```
The time series is still not stationary even after Box-Cox transformation, so we will perform first-order differencing. 

```{r}
# First-order differencing
diff_boxcox_exports <- diff(transformed_exports)
```

```{r}
# Plot ACF and PACF of the differenced series
acf2(diff_boxcox_exports, 25, main="ACF and PACF of Differenced Box-Cox Transformed Export Data")
```
After performing first-order differencing on the Box-Cox transformed data, we see that both the ACF and PACF fluctuate above and below zero at many lags but for the most part, does not exceed the confidence interval. This indicates that the series has random noise and is now stationary after removing trends/seasonality.

```{r}
ts.plot(diff_boxcox_exports)
```
After differencing the Box-Cox transformed series, we see that the time series is more stationary. 

```{r}
# Create the Q-Q plot of the residuals for the transformed model
residuals_transformed <- residuals(transformed_model)

qqnorm(residuals_transformed)
qqline(residuals_transformed, col = "red")
```
The residuals seemed to improve on the right side after performing the Box-Cox transformation.

# Fit an AR(3) model
```{r}
ar3_model = sarima(diff_boxcox_exports, 3, 0, 0)
```
The equation for the AR(3) model would be Xt = -0.3967Xt-1 - 0.2067Xt-2 + 0.2073Xt-3 + Wt, where Wt ∼ N(0, 0.2788).

```{r}
arma_model = sarima(diff_boxcox_exports, 1, 0, 1) # ARMA(1, 1) model
```
The equation for the ARMA(1, 1) model would be Xt = -0.1022Xt-1 - 0.3355Wt-1 + Wt, where Wt ∼ N(0, 0.3102). 

We see that the AR(3) model has lower AIC and BIC values, compared to the ARMA(1, 1) model, which indicates that the AR(3) model is a better fit.

```{r}
ar3_model <- arima(diff_boxcox_exports, order = c(3, 0, 0))
acf2(residuals(ar3_model), 25, main="ACF and PACF of Residuals for AR(3) Model")
```

The residuals seem to be white noise because the ACF values are mostly close to 0 and lie within the confidence interval and there are no significant spikes beyond the bounds of the intervals.

When comparing the residual analyses of both the AR(3) model and the ARMA(1, 1), we see that the AR(3) model is a lot better. When comparing the ACF plots and Ljung Box tests of their respective residuals, we can see that the ARMA(1, 1) model has points that show significance while the AR(3) model does not. This indicates that the ARMA(1, 1) model's errors do not resemble white noise which indicates a bad model fit. The AR(3) model on the other hand passes both inspections and seems to be a strong fit therefore we will elect to continue with our AR(3) model as our final model.


# Forecast
```{r}
# We forecast the next couple points using our AR(3) model

pred <- predict(ar3_model, n.ahead=5)$pred 
pred_se <- predict(ar3_model, n.ahead=5)$se

# First we must reverse the differencing on our forecasts
pred_undiffed <- pred + transformed_exports[length(transformed_exports)]
pred_se_undiffed <- pred_se + transformed_exports[length(transformed_exports)]

# Now we reverse the box-cox transformations
pred_undiffed_untransformed <- ((pred_undiffed * opt_lambda) + 1)^(1/opt_lambda)
pred_se_undiffed_untransformed <- ((pred_se_undiffed * opt_lambda) + 1)^(1/opt_lambda)

# We create the upper and lower bounds of our forecasted points' confidence intervals
upper_bound <- ts(pred_undiffed_untransformed + pred_se_undiffed_untransformed, start = 2018)
lower_bound <- ts(pred_undiffed_untransformed - pred_se_undiffed_untransformed, start = 2018)
xx = c(time(upper_bound), rev(time(lower_bound)))
yy = c(lower_bound, rev(upper_bound))

# We then plot our original export data along with our reversed forecasts and their respective confidence intervals
ts.plot(ts(export_ts$Exports, start=1960), ts(pred_undiffed_untransformed, start=2018), col=1:2)
polygon(xx, yy, border = 8, col = gray(0.8, alpha = 0.2))
lines(ts(pred_undiffed_untransformed, start=2018), type="p", col=2)
```
```{r}
predicted_values <- pred_undiffed_untransformed

# Since you predicted for 5 years ahead starting from 2018, we can assign the predicted values to the years 2018-2022
predicted_values_2018_2022 <- data.frame(
  Year = 2018:2022,
  Predicted = predicted_values
)

# Print the predicted values for 2018-2022
predicted_values_2018_2022
```